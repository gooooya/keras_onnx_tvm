{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0792cf17-dc96-42f2-9f78-21fd9f5491ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69292e-1c9c-43e9-9f46-7fd145799a3c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda4bfb2-ce3e-440c-84a7-740375966528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 01:44:36.358375: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 01:44:36.358420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 01:44:36.359189: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 01:44:36.364816: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 01:44:37.105197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-15 01:44:38.414742: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.430412: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.430470: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.434075: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.434166: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.434201: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.560780: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.560837: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.560847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-15 01:44:38.560881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:44:38.560898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4093 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 01:44:40.024675: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd78e2ee6d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-15 01:44:40.024715: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-01-15 01:44:40.029790: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-15 01:44:40.044114: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705283080.100339     546 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.2941 - accuracy: 0.9140\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1394 - accuracy: 0.9588\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1061 - accuracy: 0.9683\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0854 - accuracy: 0.9731\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0727 - accuracy: 0.9776\n",
      "313/313 - 1s - loss: 0.0730 - accuracy: 0.9772 - 803ms/epoch - 3ms/step\n",
      "\n",
      "Test accuracy: 0.9771999716758728\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# データセットの読み込み（ここではMNISTデータセットを使用）\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# データの前処理\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# モデルの定義\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# モデルの訓練\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "# モデルの評価\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac059e0-b465-4cc8-b122-0b0124ca1367",
   "metadata": {},
   "source": [
    "### onnx形式にエクスポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d071f98f-c55f-4569-8554-03d0b8479e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_tensorflow.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e82eb2c-b600-4eb6-b048-7999fc546800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not search for non-variable resources. Concrete function internal representation may have changed.\n",
      "2024-01-15 01:45:10.132508: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.132551: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-01-15 01:45:10.132680: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-01-15 01:45:10.133169: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.133228: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.133271: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.133467: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.133479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-15 01:45:10.133510: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.133523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4093 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-01-15 01:45:10.158915: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.159007: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.159061: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.159455: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.159512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-15 01:45:10.159600: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.159616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4093 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-01-15 01:45:10.163847: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.163886: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-01-15 01:45:10.164078: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-01-15 01:45:10.164590: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.164626: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.164652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.164802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.164812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-15 01:45:10.164840: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 01:45:10.164851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4093 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "# TensorFlowモデルをONNX形式に変換\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model)\n",
    "\n",
    "# ONNXモデルをファイルに保存\n",
    "onnx.save_model(onnx_model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71437cbb-c21e-4f18-bcf0-95fc6df6b2ef",
   "metadata": {},
   "source": [
    "### onnx形式のモデルの読み込みおよび確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5134475c-7ce7-4485-b442-0821692372ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力ノードの情報:\n",
      "名前: flatten_input\n",
      "形状: dim {\n",
      "  dim_param: \"unk__8\"\n",
      "}\n",
      "dim {\n",
      "  dim_value: 28\n",
      "}\n",
      "dim {\n",
      "  dim_value: 28\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 入力ノードの名前と形状情報を確認\n",
    "print(\"\\n入力ノードの情報:\")\n",
    "for input_node in onnx_model.graph.input:\n",
    "    print(\"名前:\", input_node.name)\n",
    "    print(\"形状:\", input_node.type.tensor_type.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e9a21-a0ab-4371-8ab9-42f60494159e",
   "metadata": {},
   "source": [
    "### so形式にエクスポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922f02a2-98de-41df-969f-e2f1ebac2da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "# CPU最適化\n",
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "# ONNXモデルの読み込み\n",
    "onnx_model = onnx.load(model_name)\n",
    "\n",
    "# モデルをTVMの中間表現に変換\n",
    "target = \"llvm\"\n",
    "input_shape = (1, 1, 28, 28)\n",
    "shape_dict = {input_node.name: input_shape}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# コンパイル\n",
    "with tvm.transform.PassContext(opt_level=1):\n",
    "    compiled_lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# コンパイルされたモデルの保存\n",
    "compiled_lib.export_library(\"keras-tvm-cpu.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab437951-1b05-4ed5-841c-20150760b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16.dev0\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "print(tvm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe3b257-e139-4333-9a71-6e606beb217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU最適化\n",
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "# ONNXモデルの読み込み\n",
    "onnx_model = onnx.load(model_name)\n",
    "\n",
    "# モデルをTVMの中間表現に変換\n",
    "target = \"cuda\"  # NVIDIA GPUを使用\n",
    "input_shape = (1, 1, 28, 28)\n",
    "shape_dict = {input_node.name: input_shape}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# コンパイル\n",
    "with tvm.transform.PassContext(opt_level=1):\n",
    "    compiled_lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# コンパイルされたモデルの保存\n",
    "compiled_lib.export_library(\"keras-tvm-gpu.so\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89131f-e00f-47b5-8318-c1f22d43b392",
   "metadata": {},
   "source": [
    "### 形式比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a66c3ee-602b-451b-ada5-1d56f7983d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# テストデータの前処理（適当なデータを生成）\n",
    "input_data = np.random.rand(1, 28, 28).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa6f3914-9692-4d88-b639-0d1440de499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "# ONNXモデルのパス\n",
    "onnx_model_path = 'model_tensorflow.onnx'\n",
    "\n",
    "# ONNXモデルを読み込む\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# ONNXランタイムをセットアップ\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# ONNX形式に変換したモデルでの推論\n",
    "onnx_output = ort_session.run(None, {input_node.name: input_data})\n",
    "\n",
    "# keras.Sequential形式のモデルの推論\n",
    "keras_output = model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4a72985-3bfd-4f3a-a27d-5ba6c5d234d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "\n",
    "# コンパイルされたモジュールをロード\n",
    "lib = tvm.runtime.load_module(\"keras-tvm-cpu.so\")\n",
    "dev = tvm.cpu(0)\n",
    "\n",
    "# GraphModuleを作成\n",
    "gmodc = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "# 入力を設定して実行\n",
    "gmodc.set_input(input_node.name, tvm.nd.array(input_data))\n",
    "gmodc.run()\n",
    "\n",
    "# 出力を取得\n",
    "tvm_output_cpu = gmodc.get_output(0).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb211f1-37e4-480f-9641-20fe6b2ebebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "\n",
    "# コンパイルされたモジュールをロード\n",
    "lib = tvm.runtime.load_module(\"keras-tvm-gpu.so\")\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "# GraphModuleを作成\n",
    "gmodg = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "# 入力を設定して実行\n",
    "gmodg.set_input(input_node.name, tvm.nd.array(input_data))\n",
    "gmodg.run()\n",
    "\n",
    "# 出力を取得\n",
    "tvm_output_gpu = gmodg.get_output(0).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed33b1c7-a345-4e44-8862-e998e4b5632c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keras Output</th>\n",
       "      <th>ONNX Output</th>\n",
       "      <th>tvm Output-cpu</th>\n",
       "      <th>tvm Output-gpu</th>\n",
       "      <th>onnx_output Difference</th>\n",
       "      <th>tvm_output_cpu Difference</th>\n",
       "      <th>tvm_output_gpu Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.470961</td>\n",
       "      <td>-8.470960</td>\n",
       "      <td>-8.470962</td>\n",
       "      <td>-8.470961</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>9.536743e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-23.718079</td>\n",
       "      <td>-23.718092</td>\n",
       "      <td>-23.718086</td>\n",
       "      <td>-23.718079</td>\n",
       "      <td>1.335144e-05</td>\n",
       "      <td>7.629395e-06</td>\n",
       "      <td>1.335144e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.338314</td>\n",
       "      <td>0.338309</td>\n",
       "      <td>0.338316</td>\n",
       "      <td>0.338313</td>\n",
       "      <td>5.424023e-06</td>\n",
       "      <td>2.086163e-06</td>\n",
       "      <td>4.708767e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.017379</td>\n",
       "      <td>2.017372</td>\n",
       "      <td>2.017377</td>\n",
       "      <td>2.017378</td>\n",
       "      <td>6.198883e-06</td>\n",
       "      <td>1.430511e-06</td>\n",
       "      <td>5.722046e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-29.983334</td>\n",
       "      <td>-29.983330</td>\n",
       "      <td>-29.983328</td>\n",
       "      <td>-29.983332</td>\n",
       "      <td>3.814697e-06</td>\n",
       "      <td>5.722046e-06</td>\n",
       "      <td>1.907349e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.280708</td>\n",
       "      <td>9.280710</td>\n",
       "      <td>9.280706</td>\n",
       "      <td>9.280709</td>\n",
       "      <td>1.907349e-06</td>\n",
       "      <td>1.907349e-06</td>\n",
       "      <td>9.536743e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-15.105251</td>\n",
       "      <td>-15.105246</td>\n",
       "      <td>-15.105249</td>\n",
       "      <td>-15.105250</td>\n",
       "      <td>5.722046e-06</td>\n",
       "      <td>1.907349e-06</td>\n",
       "      <td>4.768372e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.330610</td>\n",
       "      <td>3.330611</td>\n",
       "      <td>3.330606</td>\n",
       "      <td>3.330610</td>\n",
       "      <td>1.430511e-06</td>\n",
       "      <td>4.529953e-06</td>\n",
       "      <td>1.907349e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-11.947640</td>\n",
       "      <td>-11.947638</td>\n",
       "      <td>-11.947636</td>\n",
       "      <td>-11.947641</td>\n",
       "      <td>2.861023e-06</td>\n",
       "      <td>4.768372e-06</td>\n",
       "      <td>3.814697e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-8.173767</td>\n",
       "      <td>-8.173758</td>\n",
       "      <td>-8.173763</td>\n",
       "      <td>-8.173768</td>\n",
       "      <td>9.536743e-06</td>\n",
       "      <td>3.814697e-06</td>\n",
       "      <td>1.049042e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Keras Output  ONNX Output  tvm Output-cpu  tvm Output-gpu  \\\n",
       "0     -8.470961    -8.470960       -8.470962       -8.470961   \n",
       "1    -23.718079   -23.718092      -23.718086      -23.718079   \n",
       "2      0.338314     0.338309        0.338316        0.338313   \n",
       "3      2.017379     2.017372        2.017377        2.017378   \n",
       "4    -29.983334   -29.983330      -29.983328      -29.983332   \n",
       "5      9.280708     9.280710        9.280706        9.280709   \n",
       "6    -15.105251   -15.105246      -15.105249      -15.105250   \n",
       "7      3.330610     3.330611        3.330606        3.330610   \n",
       "8    -11.947640   -11.947638      -11.947636      -11.947641   \n",
       "9     -8.173767    -8.173758       -8.173763       -8.173768   \n",
       "\n",
       "   onnx_output Difference  tvm_output_cpu Difference  \\\n",
       "0            9.536743e-07               9.536743e-07   \n",
       "1            1.335144e-05               7.629395e-06   \n",
       "2            5.424023e-06               2.086163e-06   \n",
       "3            6.198883e-06               1.430511e-06   \n",
       "4            3.814697e-06               5.722046e-06   \n",
       "5            1.907349e-06               1.907349e-06   \n",
       "6            5.722046e-06               1.907349e-06   \n",
       "7            1.430511e-06               4.529953e-06   \n",
       "8            2.861023e-06               4.768372e-06   \n",
       "9            9.536743e-06               3.814697e-06   \n",
       "\n",
       "   tvm_output_gpu Difference  \n",
       "0               9.536743e-07  \n",
       "1               1.335144e-05  \n",
       "2               4.708767e-06  \n",
       "3               5.722046e-06  \n",
       "4               1.907349e-06  \n",
       "5               9.536743e-07  \n",
       "6               4.768372e-06  \n",
       "7               1.907349e-06  \n",
       "8               3.814697e-06  \n",
       "9               1.049042e-05  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ONNX出力をNumPy配列に変換して1次元に平坦化\n",
    "onnx_output = np.array(onnx_output).flatten()\n",
    "\n",
    "# Keras出力が複数次元の場合は1次元に平坦化\n",
    "keras_output = keras_output.flatten()\n",
    "\n",
    "# Keras出力が複数次元の場合は1次元に平坦化\n",
    "tvm_output_cpu = tvm_output_cpu.flatten()\n",
    "tvm_output_gpu = tvm_output_gpu.flatten()\n",
    "\n",
    "# 元は同じモデルだが推論結果が微妙に異なる(最適化などの影響の模様)\n",
    "df = pd.DataFrame({\n",
    "    'Keras Output': keras_output,\n",
    "    'ONNX Output': onnx_output,\n",
    "    'tvm Output-cpu': tvm_output_cpu,\n",
    "    'tvm Output-gpu': tvm_output_gpu,\n",
    "    'onnx_output Difference': np.abs(keras_output - onnx_output),\n",
    "    'tvm_output_cpu Difference': np.abs(keras_output - tvm_output_cpu),\n",
    "    'tvm_output_gpu Difference': np.abs(onnx_output - tvm_output_gpu)\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c1705-5aa4-4a6b-b71d-1754e981a9bb",
   "metadata": {},
   "source": [
    "### 実行時間比較"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71808415-0bff-4fb3-bc25-13c25672d806",
   "metadata": {},
   "source": [
    "input_data = np.random.rand(1, 28, 28).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2f95fea-6844-4b9d-bd22-59c0e4b16605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.9 ms ± 838 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# kerasをそのまま実行\n",
    "output = model.predict(input_data, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0821fedb-50e8-47f2-a5ff-f09dcde85687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 µs ± 211 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# ONNXモデルを実行\n",
    "output = ort_session.run(None, {input_node.name: input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29e4b021-3b12-4674-9343-6e46d4bdab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodc.set_input(input_node.name, tvm.nd.array(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0169184-a9d0-44e6-89fb-23b8cbdc5422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.2 µs ± 3.07 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# tvmモデル(cpu)を実行\n",
    "gmodc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b1085b3-f67b-4c68-be16-91df6f4a1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodg.set_input(input_node.name, tvm.nd.array(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7663471c-2bc7-4591-9f68-8c1dce4fe6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.7 µs ± 644 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# tvmモデル(gpu)を実行\n",
    "# 入力が小さい場合、GPUに転送するコストのほうが大きくなる\n",
    "gmodg.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0011afb0-8dcb-475a-bf98-69e14d4ea26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow実行の手動終了\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
